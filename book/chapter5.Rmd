---
title: "Multivariate data analysis"
output: html_fragment
---

## 3-way contingency tables with tabyl

Use the `tabyl()` function from the `janitor` package. With several adorning functions you can adjust the table to your liking. You can force the table displaying the values 'none', 'some' and 'many' in the correct order, by making sure the variable is stored as an 'ordered factor'.

```{r tabyl 3-way, echo=T, eval=F}

library(janitor)
mtcars %>% 
  tabyl(am, gear, cyl, show_na = FALSE) %>% 
  adorn_title("combined") %>% #both var names in the title
  adorn_totals("col") %>% #column totals
  adorn_totals("row") %>% #row totals
  adorn_percentages("col") %>% #columnwise, rowwise (row), or total percentages (all)
  adorn_pct_formatting(digits = 1) %>%
  adorn_ns() #show the numbers of cases

```
  
  
## Faceting of bivariate visualisations

For different subplots, use the layer `facet_wrap(~ )` as default. This is an way to introduce a third variable in the context of data visualisations.

```{r faceting, echo=T, eval=F}

mpg %>% 
  ggplot(aes(x = factor(cyl), y = hwy)) +
  geom_boxplot() + 
  facet_wrap(~ year)

```
  
  
## Linear models

For the ordinary linear model, we use `lm()`. For a clear presentation of the regression table, we use the `tidy()` function from the `broom` package. 

```{r lm, echo=T, eval=F}

library(broom)

# multiple linear regression model (no interaction)
# y = qsec, x1 = wt, x2 = cyl
# data set: mtcars

model <- mtcars %>% 
  lm(qsec ~ wt + cyl, data = .) 

# the regression table  
model %>% 
  tidy()

```

For an ANOVA table, we also use the `tidy()` function.

```{r anova, echo=T, eval=F}

model %>% 
  anova() %>% 
  tidy()

```

To obtain the R-squared, we can use the `summary()` function.

```{r r_squared, echo=T, eval=F, message=F}

out <- mtcars %>% 
  lm(qsec ~ wt + cyl, data = .) %>% 
  summary() 

# a part of subset of the thus created object 'out' is now the (adjusted) R squared.
out$r.squared
out$adj.r.squared

```

## Diagnostics

There are various ways to create plots with the residuals and/or the predicted values.

```{r residual_plots, echo=T, eval=F}
# always create a model you want to diagnose first

model <- mtcars %>% 
  lm(qsec ~ wt + cyl, data = .) 

# the object 'model' now contains the residuals and the predicted values.
# residuals and predicted values can  be added to the data frame in two ways:

# directly
mtcars$residuals <- model$residuals
mtcars$predictions <- model$fitted.values

# or by using the modelr package:
library(modelr)
mtcars <- mtcars %>% 
  add_predictions(model) %>% 
  add_residuals(model)

# after adding the residuals and the predicted values displaying residuals plots is done in the ggplot framework
mtcars %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point()

```

There are various ways to test for unequal variances in a model. These are available in various packages, including `car` and in `lmtest`.

```{r variance tests, echo=T, eval=F}

# Levene's test
library(car)
leveneTest(qsec ~ as.factor(cyl), mtcars)

# Breusch-Pagan Test
library(lmtest)
# estimate the model first and then:
bptest(model)

```

For outliers and influential cases, there are various options.

```{r outliers, echo=T, eval=F}

# after estimating and storing the model, cooks distances can be studied using
cooks.distance(model)

# also, one of the plots of the model contains info about Cook's distances (and some more)
plot(model)

```
  
After estimating the model, detecting multicollinearity can be done using the `car` library.

```{r multicollinearity, echo=T, eval=F}

library(car)
vif(model)


``` 
  
## linear mixed models

For the linear mixed model, we use the `lmer()` function from the `lme4` package. Note that the output does not show *p*-values, nor residual degrees of freedom for fixed effects. This is for a good [reason](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html). 

```{r lmer, echo=T, eval=F, message=F}

library(lme4)

mtcars %>% 
  lmer(qsec ~ wt + (1|gear), data = .) %>% 
  tidy()

# When we have factors as fixed variables:
mtcars %>% 
  lmer(qsec ~ wt + factor(cyl) + (1|gear), data = .) %>% 
  anova() %>% 
  tidy()

```

If you want approximate *p*-values, you can use Satterthwaite's degrees of freedom method, implemented in the `lmerTest` package. The same method is used in SPSS. 

```{r, eval=F}

library(lmerTest)
mtcars %>% 
  lmer(qsec ~ wt + (1|gear), data = .) %>% 
  summary()

```

For a residual plot, we can use similar syntax as for the linear model:

```{r lmer_plot, echo=T, eval=F}

model <- mtcars %>% 
  lmer(qsec ~ wt + (1|gear), data = .)  

mtcars %>% 
  add_predictions(model) %>% 
  add_residuals(model) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point()

```

## logistic regression

For a logistic regression model, we use the `glm()` function. 

```{r logistic, echo=T, eval=F}

mtcars %>% 
  glm(am ~ wt, family = binomial, data = .) %>% 
  tidy()

```

Similarly for a Poisson regression. 

```{r poisson, echo=T, eval=F}

mtcars %>% 
  glm(carb ~ wt, family = poisson, data = .) %>% 
  tidy()

```


## Nonparametric tests

For a non-parametric test for repeated measures, we use 

```{r friedman, echo=T, eval=F}

iris %>% 
  select(Sepal.Length, Petal.Length) %>% 
  as.matrix() %>%
  friedman.test()

```
